{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnplY6LgVkqU"
      },
      "source": [
        "### PDF Scraper\n",
        "By Parker Whitehead<br/><br/>\n",
        "\n",
        "This jupyter notebook is a repurposing of the 'NEW HPC' notebook that has been commonly used to scrape websites for ConfliBERT. Although it is currently set up for a very niche and specific usecase (mining the online library of pdfs from https://www.corteidh.or.cr/), it can be modified to meet any pdf scraping need.<br/><br/>\n",
        "\n",
        "The notebook is currently broken into **4 main parts.** The **first** is setting your universal parameters. This is the same as usual, except there are a few new ones for pdf specific mining. The **second** part is a page-based scrape for article links given a specific endpoint. In this instance, it is collecting links to all pdf descriptions under the category of 'human rights,' or 'derechos humanos.' On a typical website, this would be sufficient to find the links to PDF downloads. <br/><br/>\n",
        "\n",
        "\n",
        "**However**, this website uses a PDF viewer and requires a login. As such, I am using a workaround where I collect the links of pdf descriptions, which contain the title of the pdf as well as the ID of the pdf file. After doing some digging in the javascript code, I found out that the pdf viewer was hitting an endpoint on the server that contained the raw PDF file. Conveniently, it was not protected, meaning that you didn't have to log in or meet any pay wall that you would if you tried to access the pdf normally thorugh the viewer. The endpoint for the pdfs was indexable given that you had the pdf ID.<br/><br/>\n",
        "\n",
        "This is the purpose behind the **third part,** which scrapes the pdf descriptions not for links to the pdf or for raw text, but for the pdf title and ID. There is also an **intermediate step** where a library is used to ensure that the title is in spanish, as there are a fair number of english articles. The **fourth** is the actual PDF scraping.<br/><br/>\n",
        "\n",
        "Every website is going to be different, so it is almost certian that you are going to have to modify this script in some way. However, it is generally as simple as finding a way to hit a websites endpoint that contain the raw PDFs. The best way that I've found to customize these notebooks is by creating new extract functions, like extract_text() and extract_text_from_df(), which are parallelized with the parallelize_dataframe function. Reading through this scraper should hopefully provide context as to how they work and how you can create your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ebfc8e7"
      },
      "outputs": [],
      "source": [
        "# Uncomment these if running in colab\n",
        "!pip install wget\n",
        "!pip install newspaper3k\n",
        "!pip install xmltodict\n",
        "!pip install pandarallel\n",
        "!pip install datefinder\n",
        "!pip install pydrive\n",
        "!pip install selenium\n",
        "!pip install PyPDF2\n",
        "!pip install langdetect\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip3 install pycryptodome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLFbEx9LcMY6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5314e252",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import wget\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "import json\n",
        "import pandas as pd\n",
        "pd.set_option(\"max_colwidth\", 600)\n",
        "import ast\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import html\n",
        "import re\n",
        "import itertools\n",
        "import lxml\n",
        "import xmltodict\n",
        "import collections\n",
        "from urllib import request\n",
        "from collections import OrderedDict\n",
        "from urllib.error import HTTPError, URLError\n",
        "from urllib.parse import urlparse\n",
        "import sys\n",
        "import ast\n",
        "import time\n",
        "from pandarallel import pandarallel\n",
        "import requests\n",
        "import datefinder\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
        "from multiprocessing import Pool\n",
        "import random\n",
        "from urllib.parse import quote\n",
        "import pytz\n",
        "from PyPDF2 import PdfReader\n",
        "import io\n",
        "import langdetect\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e7001a6"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "\n",
        "#PLEASE EDIT THIS CELL!\n",
        "\n",
        "links_file_path = \"\"\n",
        "checkpoints_file_path = \"\"\n",
        "csv_file_path = \"\"\n",
        "\n",
        "#Universal parameters. Always needs to be changed\n",
        "news_outlet = \"\"\n",
        "country = \"\"\n",
        "max_click_SHOW_MORE = 500\n",
        "host = \"\"\n",
        "date_format = \"%Y-%m-%d\"\n",
        "relevant_path = f'^{host}.*$'\n",
        "# Allow http and https links\n",
        "if re.match(\"^https?.*\", host):\n",
        "  relevant_path = f'^https?:\\/\\/{host[8:]}.*$'\n",
        "else:\n",
        "  relevant_path = f'^https?:\\/\\/{host}.*$'\n",
        "urls = ['/']\n",
        "urls = [host + url for url in urls]\n",
        "csv_name = country+'_'+news_outlet+'.csv'\n",
        "types = ['page', 'click_more','scroll_down']\n",
        "type_of_page = types[0]\n",
        "extract_text_sleep = random.randint(2,4)\n",
        "extract_urls_sleep = random.randint(2,4)\n",
        "\n",
        "# If page type is click_more or scroll_down i.e. can get more article by clicking a button with text 'click more' or simply scrolling down\n",
        "# Be sure to use xpath functions to describe the exact button you want to press.\n",
        "# Sometimes, 'previous' and 'next' for click more can have the same class.\n",
        "# You may need to do something like this:\n",
        "# \"//a[contains(@class,\\\"BlogList-pagination-link\\\") and .//span[text()=\\\"MÃ¡s antiguos\\\"]]\"\n",
        "\n",
        "xpath_for_link = \"\"\n",
        "xpath_for_click_more = \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# If page type is page\n",
        "target_tag = ''\n",
        "target_tag_class = ''\n",
        "\n",
        "page_identifier = ''\n",
        "\n",
        "if type_of_page == 'page':\n",
        "    urls = [url + page_identifier for url in urls]\n",
        "pages_each_category = [1] #The total number of pages for each section. So if policiaca is 1..92, write 92, not 93.\n",
        "if pages_each_category:\n",
        "    total_pages = pages_each_category\n",
        "else:\n",
        "    total_pages = 500\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "#Get amount of pages per category not having text code\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "retrieve_amount_of_pages_no_text = False\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#Get amount of pages per category with text code\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "retrieve_amount_of_pages_w_text = False\n",
        "xpath_for_amount_of_pages = \"\"\n",
        "index_for_amount_of_pages_location = 0\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# in case multi tags\n",
        "target_tags = []\n",
        "target_tag_classes = []\n",
        "\n",
        "# New, pdf relevant parameters:\n",
        "# Used in this specific instance to index raw pdfs once their id was obtained.\n",
        "# Might not be applicable for other usecases.\n",
        "pdf_url_endpoint = ''\n",
        "relevant_pdf_path = f'^{pdf_url_endpoint}.*$'\n",
        "# Allow http and https links\n",
        "if re.match(\"^https?.*\", pdf_url_endpoint):\n",
        "  relevant_pdf_path = f'^https?:\\/\\/{pdf_url_endpoint[8:]}.*$'\n",
        "else:\n",
        "  relevant_pdf_path = f'^https?:\\/\\/{pdf_url_endpoint}.*$'\n",
        "\n",
        "intermediate_site = False # IMPORTANT, CHANGE\n",
        "\n",
        "title_tag = ''\n",
        "title_class = ''\n",
        "\n",
        "# Used if there is an intermediate site:\n",
        "\n",
        "pdf_id_tag = ''\n",
        "pdf_id_class = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPVY5bHd6gM5"
      },
      "outputs": [],
      "source": [
        "def parallelize_dataframe(df, func, n_cores):\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "    pool = Pool(n_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byiwFvDpx02v"
      },
      "outputs": [],
      "source": [
        "# Functions for automatically identifying the number of pages\n",
        "\n",
        "def check_it_has_links(url):\n",
        "  options = FirefoxOptions()\n",
        "  options.add_argument(\"--headless\")\n",
        "  driver = webdriver.Firefox(options=options)\n",
        "  driver.get(url)\n",
        "\n",
        "  has_links= True\n",
        "  try:\n",
        "    element = WebDriverWait(driver, 5).until(\n",
        "        EC.presence_of_element_located((By.XPATH, xpath_for_link))\n",
        "    )\n",
        "  except TimeoutException as e:\n",
        "    print('Could not find pages count')\n",
        "    has_links = False\n",
        "  driver.quit()\n",
        "  return has_links\n",
        "\n",
        "def retrieve_pages_per_category_no_text(url):\n",
        "  pages = 1\n",
        "  pages_defined = False\n",
        "\n",
        "  while check_it_has_links(url + f'{pages}'):\n",
        "    pages = pages * 10\n",
        "\n",
        "  add_pages = pages // 2\n",
        "  if pages != 1:\n",
        "    pages = pages // 2\n",
        "    while not pages_defined:\n",
        "      add_pages = add_pages // 2\n",
        "      if check_it_has_links(url + f'{pages}'):\n",
        "        pages = add_pages + pages\n",
        "      else:\n",
        "        pages = pages - add_pages\n",
        "      if add_pages == 1:\n",
        "        pages_defined = True\n",
        "      print(pages)\n",
        "\n",
        "  return pages\n",
        "\n",
        "def retrieve_pages_per_category_w_text(url):\n",
        "  options = FirefoxOptions()\n",
        "  options.add_argument(\"--headless\")\n",
        "  driver = webdriver.Firefox(options=options)\n",
        "  driver.get(url)\n",
        "  try:\n",
        "    element = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.XPATH, xpath_for_amount_of_pages))\n",
        "    )\n",
        "  except TimeoutException as e:\n",
        "    print('Could not find pages count')\n",
        "    return ''\n",
        "  pages = driver.find_element(By.XPATH, xpath_for_amount_of_pages).text\n",
        "  if not pages:\n",
        "    print('No text found')\n",
        "  driver.quit()\n",
        "  return pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pjV7ws-x0qU"
      },
      "outputs": [],
      "source": [
        "if retrieve_amount_of_pages_no_text:\n",
        "  total_pages = []\n",
        "  for url in urls:\n",
        "    amount_of_pages = retrieve_pages_per_category_no_text(url)\n",
        "    total_pages.append(amount_of_pages)\n",
        "  print(total_pages)\n",
        "\n",
        "if retrieve_amount_of_pages_w_text:\n",
        "  total_pages = []\n",
        "  for url in urls:\n",
        "    text_with_amount_of_pages = retrieve_pages_per_category_w_text(url)\n",
        "\n",
        "    amount_of_pages =[]\n",
        "\n",
        "    for char in text_with_amount_of_pages[index_for_amount_of_pages_location:]:\n",
        "      if not char.isnumeric():\n",
        "        if char != ',' and char != '.':\n",
        "          break\n",
        "      else:\n",
        "        amount_of_pages.append(char)\n",
        "    amount_of_pages = ''.join(amount_of_pages)\n",
        "    print(amount_of_pages + ' ' + url)\n",
        "    if amount_of_pages:\n",
        "      total_pages.append(int(amount_of_pages))\n",
        "    else:\n",
        "      total_pages.append(1)\n",
        "  print(total_pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9cf2819"
      },
      "outputs": [],
      "source": [
        "\n",
        "output_path = 'links_'+csv_name\n",
        "\n",
        "def retrieve_links_from_list(url,scroll_down=False):\n",
        "  options = FirefoxOptions()\n",
        "  options.add_argument(\"--headless\")\n",
        "  driver = webdriver.Firefox(options=options)\n",
        "  driver.get(url)\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  all_links = []\n",
        "\n",
        "  news_urls = set()\n",
        "\n",
        "  prev_length = 0\n",
        "  while count <= max_click_SHOW_MORE:\n",
        "      if not scroll_down:\n",
        "          try:\n",
        "            element = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.XPATH, xpath_for_click_more))\n",
        "            )\n",
        "          except TimeoutException as e:\n",
        "            print('Could not find click more button')\n",
        "            break\n",
        "\n",
        "\n",
        "      try:\n",
        "          all_links = driver.find_elements(By.XPATH, xpath_for_link)\n",
        "          for link in all_links:\n",
        "            if isinstance(link.get_attribute('href'), str):\n",
        "              if re.match(relevant_pdf_path, link.get_attribute('href')):\n",
        "                news_urls.add(link.get_attribute('href'))\n",
        "\n",
        "\n",
        "          if not scroll_down:\n",
        "            click_more = driver.find_element(By.XPATH, xpath_for_click_more)\n",
        "          curr_length = len(all_links)\n",
        "          print(f'{curr_length} links in current page')\n",
        "          print(f'{prev_length} links in previous page')\n",
        "          if count > 0:\n",
        "            if scroll_down and (curr_length == prev_length or (not scroll_down and not click_more)):\n",
        "              break\n",
        "\n",
        "          current_links = [l.get_attribute(\"href\") for l in all_links[prev_length:]]\n",
        "          df_link = pd.DataFrame(current_links, columns = ['link'])\n",
        "          df_link.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
        "\n",
        "          prev_length = curr_length\n",
        "\n",
        "          if not scroll_down:\n",
        "\n",
        "            try:\n",
        "                click_more.click();\n",
        "            except Exception as e:\n",
        "                driver.execute_script(\"arguments[0].click();\", click_more) #If click does not work because of overlapping elements, this executes\n",
        "\n",
        "            print(f\"Button clicked {count} times\", )\n",
        "          else:\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            print(f\"Scrolled down {count+1} times\")\n",
        "\n",
        "          time.sleep(3)\n",
        "\n",
        "          count += 1\n",
        "\n",
        "\n",
        "\n",
        "      except TimeoutException:\n",
        "          break\n",
        "      except NoSuchElementException:\n",
        "          break\n",
        "\n",
        "  time.sleep(2)\n",
        "\n",
        "  all_links = driver.find_elements(By.XPATH, xpath_for_link)\n",
        "\n",
        "  for link in all_links:\n",
        "        if isinstance(link.get_attribute('href'), str):\n",
        "          if re.match(relevant_pdf_path, link.get_attribute('href')):\n",
        "            news_urls.add(link.get_attribute('href'))\n",
        "\n",
        "  driver.quit()\n",
        "  print('*' * 20)\n",
        "  return list(news_urls)\n",
        "\n",
        "\n",
        "def extract_urls(url):\n",
        "    rows = set()\n",
        "    soup = getSoup(url)\n",
        "    if soup:\n",
        "      all_elems = None\n",
        "      if target_tag_class != '':\n",
        "        all_elems = soup.find_all(target_tag, {'class':target_tag_class})\n",
        "      else:\n",
        "        all_elems = soup.find_all(target_tag)\n",
        "\n",
        "      for d in all_elems:\n",
        "        all_links = d.find_all('a', href=True)\n",
        "        # If there are no links, try to grab href from found element\n",
        "        if not all_links:\n",
        "          try:\n",
        "            # MADE CHANGE FOR SPECIFIC WEBSITE\n",
        "            if not re.match('^http.*', d['href']):\n",
        "              # d['href'] = host + d['href']\n",
        "              d['href'] = \"https://\" + d['href'][2:]\n",
        "            rows.add(d['href'])\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "        for l in all_links:\n",
        "            if not re.match('^http.*', l['href']):\n",
        "              # l['href'] = host + l['href']\n",
        "              l['href'] = pdf_url_endpoint + l['href']\n",
        "            if re.match(relevant_pdf_path, l['href']):\n",
        "              rows.add(l['href'])\n",
        "\n",
        "\n",
        "    if rows:\n",
        "      return list(rows)\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "def extract_urls_from_df(df):\n",
        "    links = df['page'].map(lambda x: extract_urls(x))\n",
        "    df['link'] = links\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ecec80d"
      },
      "outputs": [],
      "source": [
        "pages = []\n",
        "\n",
        "def get_zipped_urls(url, total_pages):\n",
        "    page_numbers = [str(i) for i in range(1,total_pages)]\n",
        "    url_multiple = [url] * total_pages\n",
        "    return [''.join(x) for x in zip(url_multiple,page_numbers)]\n",
        "\n",
        "if type_of_page == 'page':\n",
        "    for ind, url in enumerate(urls):\n",
        "        if isinstance(total_pages, int):\n",
        "            pages += get_zipped_urls(url, total_pages)\n",
        "        else:\n",
        "            pages_for_category = total_pages[ind] + 1\n",
        "            pages += get_zipped_urls(url, pages_for_category)\n",
        "\n",
        "elif type_of_page == 'click_more' or type_of_page == 'scroll_down':\n",
        "    for url in urls:\n",
        "        pages += retrieve_links_from_list(url, True if type_of_page == 'scroll_down' else False)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(pages, columns = ['page'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0755f971"
      },
      "outputs": [],
      "source": [
        "def getSoup(url):\n",
        "    \"\"\"\n",
        "    Return a soup object of the URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        req = request.Request(url, headers={'User-Agent' : \"Chrome\"})\n",
        "        con = request.urlopen(req)\n",
        "        time.sleep(extract_urls_sleep)\n",
        "        html = con.read()\n",
        "\n",
        "    except HTTPError as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "    except URLError as e:\n",
        "        print('The server could not be found')\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return None\n",
        "\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78105dd7"
      },
      "outputs": [],
      "source": [
        "# Contains new extracting methods\n",
        "\n",
        "def extract_text(url):\n",
        "    text, title, date = None, None, None\n",
        "    try:\n",
        "        result = [country, news_outlet] + [''] * 3\n",
        "        article = Article(url, keep_article_html=False)\n",
        "        article.download()\n",
        "        time.sleep(extract_text_sleep)\n",
        "        article.parse()\n",
        "        text = article.text\n",
        "\n",
        "        if text:\n",
        "            text_copy = text\n",
        "            title = article.title\n",
        "            date = article.publish_date\n",
        "            if date:\n",
        "                date = date.strftime(date_format)\n",
        "            else:\n",
        "                matches = datefinder.find_dates(text_copy)\n",
        "                most_recent_datetime = sorted(matches)[-1]\n",
        "                date = most_recent_datetime.strftime(date_format)\n",
        "    finally:\n",
        "      if title:\n",
        "        result[2] = title\n",
        "      if date:\n",
        "        result[3] = date\n",
        "      if text:\n",
        "        result[4] = text\n",
        "      return result\n",
        "\n",
        "\n",
        "def extract_text_from_df(df):\n",
        "    content = df['link'].map(lambda x: extract_text(x))\n",
        "    df['content'] = content\n",
        "    return df\n",
        "\n",
        "\n",
        "def extract_id_and_title(url):\n",
        "    title_text, id_tag = None, None\n",
        "    try:\n",
        "      time.sleep(random.randint(4,6))\n",
        "      current_soup = getSoup(url)\n",
        "      # CHANGE FOR INTERMEDIATE WEBSITE\n",
        "      try:\n",
        "        title_text = current_soup.find(title_tag, class_=title_class).get_text()\n",
        "      except:\n",
        "        title_text = url[url.rindex('/')+1:]\n",
        "      id_tags = current_soup.find_all(pdf_id_tag, class_=pdf_id_class)\n",
        "      id_tag = None\n",
        "      for possible_id_tag in id_tags:\n",
        "        if re.match(relevant_pdf_path, possible_id_tag.get('href')):\n",
        "          id_tag = possible_id_tag.get('href')\n",
        "        else:\n",
        "          id_tag = pdf_url_endpoint + possible_id_tag.get('href')\n",
        "      if not id_tag:\n",
        "        title_text = url[url.rindex('/')+1:]\n",
        "        id_tag = url\n",
        "      # id_tag = id_tag[id_tag.rindex('/')+1:]\n",
        "    except Exception as e:\n",
        "        print(f'Error in extract_id_and_title: {e}')\n",
        "\n",
        "    finally:\n",
        "        results = (title_text, id_tag)\n",
        "        return results\n",
        "\n",
        "def extract_id_and_title_from_df(df):\n",
        "    try:\n",
        "        content = df['link'].map(lambda x: extract_id_and_title(x))\n",
        "        if content.tolist(): # Detects if weird race condition didn't happen\n",
        "          df[['title','pdf_id']] = pd.DataFrame(content.tolist(),index=content.index)\n",
        "    except Exception as e:\n",
        "        print(f'Error in extract_id_and_title_from_df: {e}')\n",
        "    return df\n",
        "\n",
        "def extract_pdf_text(x, url):\n",
        "    text = None\n",
        "    try:\n",
        "      url = url + str(int(x)) + '.pdf'\n",
        "    except:\n",
        "      url = x\n",
        "    try:\n",
        "        texts = []\n",
        "        response = requests.get(url)\n",
        "\n",
        "        with io.BytesIO(response.content) as open_pdf_file:\n",
        "            reader = PdfReader(open_pdf_file)\n",
        "            if reader.is_encrypted:\n",
        "              reader.decrypt('')\n",
        "            for pdfPage in reader.pages:\n",
        "                texts.append(pdfPage.extract_text())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error in extract_pdf_text: {e}')\n",
        "\n",
        "    if len(texts) == 0:\n",
        "        texts.append('')\n",
        "\n",
        "    return texts\n",
        "\n",
        "\n",
        "def extract_pdf_text_from_df(df):\n",
        "    if intermediate_site:\n",
        "      try:\n",
        "          content = df['pdf_id'].map(lambda x: extract_pdf_text(x, pdf_url_endpoint))\n",
        "          df['text'] = content\n",
        "      except Exception as e:\n",
        "          print(f'Error in extract_pdf_text_from_df: {e}')\n",
        "    else:\n",
        "      try:\n",
        "          content = df['link'].map(lambda x: extract_pdf_text(x, pdf_url_endpoint))\n",
        "          df['text'] = content\n",
        "      except Exception as e:\n",
        "          print(f'Error in extract_pdf_text_from_df: {e}')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f114c52c"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83ebc4cd"
      },
      "outputs": [],
      "source": [
        "def parallel_work(df, method_to_run, target, source):\n",
        "  global workers\n",
        "  while workers>0:\n",
        "    try:\n",
        "      df[target] = df[source].parallel_apply(method_to_run)\n",
        "    except Exception as e:\n",
        "      raise e\n",
        "    break\n",
        "\n",
        "  if workers == 0:\n",
        "    print('Error during parallel operation. Could not extract text')\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_parallel_operation_results(divided_dfs, method_to_run, target, source):\n",
        "  res = []\n",
        "  for df in divided_dfs:\n",
        "    try:\n",
        "      temp_df = parallel_work(df, method_to_run, target, source)\n",
        "      if temp_df[target].isnull().all():\n",
        "        print('Could not retrieve any URLS')\n",
        "        print('Something is wrong with the target_tag and target_tag_class variables. Please modify')\n",
        "        return []\n",
        "      res.append(temp_df)\n",
        "    except Exception:\n",
        "      continue\n",
        "  df_result = pd.concat(res)\n",
        "  return df_result\n",
        "\n",
        "def partition_df(df):\n",
        "  global articles_per_parallel_operation\n",
        "  divided_dfs = []\n",
        "  start = 0\n",
        "  while start < len(df):\n",
        "    divided_dfs.append(df[start:start+articles_per_parallel_operation])\n",
        "    start += articles_per_parallel_operation\n",
        "  return divided_dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ada4c2eb"
      },
      "outputs": [],
      "source": [
        "if type_of_page == 'page':\n",
        "    start = 0\n",
        "    limit = 40\n",
        "    total_time_start = time.time()\n",
        "    results = []\n",
        "\n",
        "    while start < len(df):\n",
        "        start_time = time.time()\n",
        "        results.append(parallelize_dataframe(df[start:start+limit], extract_urls_from_df, 24))\n",
        "        end_time = time.time()\n",
        "        print(f'Batch of data of row range {start}-{start+limit} complete in {round(end_time-start_time, 2)} seconds')\n",
        "        print(f'{round(min((((start+limit) / len(df)) * 100), 100), 2)}% complete')\n",
        "        start+=limit\n",
        "\n",
        "    df = pd.concat(results)\n",
        "    total_time_end = time.time()\n",
        "    print(f'total time taken: {round(total_time_end - total_time_start,2)} second')\n",
        "\n",
        "else:\n",
        "    df = df.rename(columns={\"page\": \"link\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHVfbOYU5MtL"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f604cbb"
      },
      "outputs": [],
      "source": [
        "df.to_csv(links_file_path+'links_'+csv_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR5D1Xpn2w6G"
      },
      "outputs": [],
      "source": [
        "print(len(df))\n",
        "df = df[df['link'].notna()]\n",
        "df = df[(df['link'].str.len() != 0)]\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c1ca560",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if type_of_page == 'page':\n",
        "    df = df.explode('link')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDqQExlIZ-YT"
      },
      "outputs": [],
      "source": [
        "print(len(df))\n",
        "df = df.drop_duplicates(subset='link')\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80bc38c9"
      },
      "outputs": [],
      "source": [
        "#Create a links checkpoint csv\n",
        "df.to_csv(links_file_path+'links_'+csv_name)\n",
        "#df.to_csv('/xdisk/josorio1/salsarra/links/'+'links_'+csv_name, encoding = 'utf-8-sig')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnp-nQgdfpJA"
      },
      "outputs": [],
      "source": [
        "if 'df' not in locals() or df is None or 'link' not in df:\n",
        "    if(os.path.exists(links_file_path+'links_'+csv_name)):\n",
        "        df = pd.read_csv(links_file_path+'links_'+csv_name, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToQVNKdf8eD4"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v73KqFYTeKrz"
      },
      "outputs": [],
      "source": [
        "# SKIP IF THE PDFS DON'T REQUIRE AN ENDPOINT OR AN INTERMEDIATE\n",
        "output_path= 'unprocessed_' + csv_name\n",
        "start = 0\n",
        "limit = 40\n",
        "\n",
        "res = []\n",
        "while start < len(df):\n",
        "    start_time = time.time()\n",
        "    demo_df = df[start:start+limit].copy()\n",
        "    test_df = parallelize_dataframe(demo_df, extract_id_and_title_from_df, 94)\n",
        "    res.append(test_df)\n",
        "    test_df.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
        "\n",
        "    test_df['id_found'] = test_df['pdf_id'].map(lambda x: True if x != None else False)\n",
        "    no_of_id_retrieved = test_df.id_found.sum()\n",
        "    print(f'{no_of_id_retrieved} / {len(test_df)} ids retrieved')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f'Batch of data of row range {start}-{start+limit} complete in {round(end_time-start_time, 2)} seconds')\n",
        "    print(f'{round(min((((start+limit) / len(df)) * 100), 100), 2)}% complete')\n",
        "    start+=limit\n",
        "\n",
        "df = pd.concat(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ku5T0eTATz0"
      },
      "outputs": [],
      "source": [
        "# SKIP IF THE PDFS DON'T REQUIRE AN ENDPOINT OR AN INTERMEDIATE\n",
        "\n",
        "# Data cleaning\n",
        "\n",
        "print(df.shape)\n",
        "df = df[df['id_found'] == True]\n",
        "print(df.shape)\n",
        "\n",
        "df = df[['page','link','title','pdf_id','id_found']]\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX5v0MY5Vkqf"
      },
      "outputs": [],
      "source": [
        "# Create a secondary checkpoint for ids and titles\n",
        "df.to_csv(checkpoints_file_path+'checkpoint_'+csv_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZW1ZMzhVkqf"
      },
      "outputs": [],
      "source": [
        "# Run to load from csv checkpoint\n",
        "df = pd.read_csv(checkpoints_file_path+'checkpoint_'+csv_name)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1AGPtt-Vkqg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Run parallel text scraping from pdf\n",
        "output_path= 'unprocessed_' + csv_name\n",
        "start = 0\n",
        "limit = 40\n",
        "\n",
        "res = []\n",
        "while start < len(df):\n",
        "    start_time = time.time()\n",
        "    demo_df = df[start:start+limit].copy()\n",
        "    test_df = parallelize_dataframe(demo_df, extract_pdf_text_from_df, 24)\n",
        "    res.append(test_df)\n",
        "    test_df.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
        "\n",
        "    test_df['text_found'] = test_df['text'].map(lambda x: True if len(x) > 1 else False)\n",
        "    no_of_id_retrieved = test_df.text_found.sum()\n",
        "    print(f'{no_of_id_retrieved} / {len(test_df)} text retrieved')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f'Batch of data of row range {start}-{start+limit} complete in {round(end_time-start_time, 2)} seconds')\n",
        "    print(f'{round(min((((start+limit) / len(df)) * 100), 100), 2)}% complete')\n",
        "    start+=limit\n",
        "\n",
        "df = pd.concat(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyIWi9pCXzbJ"
      },
      "outputs": [],
      "source": [
        "# df[['country', 'news_outlet', 'title', 'date', 'text']] = pd.DataFrame(df.content.tolist(), index= df.index)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAbNks4tOxpb"
      },
      "outputs": [],
      "source": [
        "print(len(df))\n",
        "df = df[df['text_found'] == True]\n",
        "print(len(df))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKU62IqWQlYi"
      },
      "outputs": [],
      "source": [
        "if 'page' in df:\n",
        "  df = df.drop(['page'], axis=1)\n",
        "if 'content' in df:\n",
        "  df = df.drop(['content'], axis=1)\n",
        "if 'Unnamed: 0' in df:\n",
        "    df = df.drop(['Unnamed: 0'], axis=1)\n",
        "if 'Unnamed: 0.1' in df:\n",
        "    df = df.drop(['Unnamed: 0.1'], axis=1)\n",
        "if 'text_found' in df:\n",
        "    df = df.drop(['text_found'], axis=1)\n",
        "if 'id_found' in df:\n",
        "    df = df.drop(['id_found'], axis=1)\n",
        "if 'pdf_id' in df:\n",
        "    df = df.drop(['pdf_id'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkdz03KWVkqg"
      },
      "outputs": [],
      "source": [
        "df['date'] = ''\n",
        "df['news_outlet'] = news_outlet\n",
        "df['country'] = country\n",
        "df['text_set'] = 'TRUE'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzkJAu_DVkqg"
      },
      "outputs": [],
      "source": [
        "correct_column_order = ['link','text_set','country','news_outlet','date','text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgKexrH4AawL"
      },
      "outputs": [],
      "source": [
        "df = df[correct_column_order]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b8e0539"
      },
      "outputs": [],
      "source": [
        "seconds = time.time() - t0\n",
        "duration = time.strftime(\"(%H:%M:%S)\",time.gmtime(seconds))\n",
        "\n",
        "df.to_csv(csv_file_path+duration+'-'+csv_name, encoding = 'utf-8-sig')\n",
        "\n",
        "print('Time to complete:',duration)\n",
        "\n",
        "#tz_DFW = pytz.timezone('US/Central')\n",
        "#current_time = datetime.now(tz_DFW)\n",
        "#time = current_time.strftime(\"(%H:%M:%S)\")\n",
        "\n",
        "\n",
        "\n",
        "#df.to_csv(time+csv_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TJOmWM2dPI_"
      },
      "outputs": [],
      "source": [
        "if(os.path.exists(output_path)):\n",
        "    os.remove(output_path)\n",
        "    print(('unprocessed file deleted'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
